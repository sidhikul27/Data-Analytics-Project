#Import BlackFriday.csv data set into R
data <- read.csv("BlackFriday.csv", sep=",", header=T)
#Replace missing values in Product_Category_2 & Product_Category_3 variables with the mean value
data$Product_Category_2[is.na(data$Product_Category_2)] <- 9.84 
data$Product_Category_3[is.na(data$Product_Category_3)] <- 12.7
data$Product_Category_1 <- as.numeric(data$Product_Category_1)
#Eliminate User_ID & Product_ID variables from the data set
data<-subset(data,select = -User_ID)
data<-subset(data,select = -Product_ID)
#Convert the Purchase variable into nominal variable using the dummy variable
data$Purchase<-ifelse(data$Purchase>9334,1,0)
data$Purchase<-as.factor(data$Purchase)
head(data)
data$Occupation<-as.factor(data$Occupation)
data$Marital_Status<-as.factor(data$Marital_Status)
#load the BSDA library to perform z tests
library(BSDA)
#Perform Z Test on Hypothesis 1
z.test(data$Product_Category_1,NULL,alternative="less",mu=5,sigma.x=sd(data$Product_Category_1),sigma.y=NULL,conf.level=0.95)
#Perform Z Test on Hypothesis 2
z.test(data$Product_Category_1,Product_Category_2,alternative="two.sided",mu=0,sigma.x=sd(data$Product_Category_1),sigma.y=(data$Product_Category_2),conf.level=0.95)
#Build Classification models
#Clone data into naive_bayes variable
data_nb<-data
#Convert the numeric variables into groups using the cut function
data_nb$Product_Category_1<-cut(data_nb$Product_Category_1,3)
data_nb$Product_Category_2<-cut(data_nb$Product_Category_2,4)
data_nb$Product_Category_3<-cut(data_nb$Product_Category_3,4)
data_nb_new<-data_nb
#load the library Naive Bayes
library(naivebayes)
#Split the data into 80% training data and 20% testing data
index.data_nb<-sample(1:nrow(data_nb),size=round(0.8*nrow(data_nb)))
train.data_nb<-data_nb[index.data_nb,]
test.data_nb<-data_nb[-index.data_nb,]
#Build naive_bayes model
model_nb<-naive_bayes(Purchase~.,train.data_nb)
#make predictions on the testing data and compute accuracy of the model
pred<-predict(model_nb,test.data_nb)
head(predict(model_nb,test.data_nb,type="prob"))
#load Metrics library to compute accuracy
library(Metrics)
accuracy(test.data_nb$Purchase,pred)
#clone naive bayes data saved earlier to try different groups and build models
data_nb<-data_nb_new
data_nb$Product_Category_3<-cut(data_nb$Product_Category_3,3)
data_nb$Product_Category_2<-cut(data_nb$Product_Category_2,2)
data_nb$Product_Category_1<-cut(data_nb$Product_Category_1,4)
index.data_nb<-sample(1:nrow(data_nb),size=round(0.8*nrow(data_nb)))
train.data_nb<-data_nb[index.data_nb,]
test.data_nb<-data_nb[-index.data_nb,]
model_nb<-naive_bayes(Purchase~.,train.data_nb)
pred<-predict(model_nb,test.data_nb)
accuracy(test.data_nb$Purchase,pred)
data_nb<-data_nb_new
data_nb$Product_Category_3<-cut(data_nb$Product_Category_3,6)
data_nb$Product_Category_2<-cut(data_nb$Product_Category_2,6)
data_nb$Product_Category_1<-cut(data_nb$Product_Category_1,2)
index.data_nb<-sample(1:nrow(data_nb),size=round(0.8*nrow(data_nb)))
train.data_nb<-data_nb[index.data_nb,]
test.data_nb<-data_nb[-index.data_nb,]
model_nb<-naive_bayes(Purchase~.,train.data_nb)
pred<-predict(model_nb,test.data_nb)
accuracy(test.data_nb$Purchase,pred)
data_nb<-data_nb_new
data_nb$Product_Category_3<-cut(data_nb$Product_Category_3,8)
data_nb$Product_Category_2<-cut(data_nb$Product_Category_2,8)
data_nb$Product_Category_1<-cut(data_nb$Product_Category_1,6)
index.data_nb<-sample(1:nrow(data_nb),size=round(0.8*nrow(data_nb)))
train.data_nb<-data_nb[index.data_nb,]
test.data_nb<-data_nb[-index.data_nb,]
model_nb<-naive_bayes(Purchase~.,train.data_nb)
pred<-predict(model_nb,test.data_nb)
accuracy(test.data_nb$Purchase,pred)
data_nb<-data_nb_new
data_nb$Product_Category_3<-cut(data_nb$Product_Category_3,8)
data_nb$Product_Category_2<-cut(data_nb$Product_Category_2,8)
data_nb$Product_Category_1<-cut(data_nb$Product_Category_1,8)
index.data_nb<-sample(1:nrow(data_nb),size=round(0.8*nrow(data_nb)))
train.data_nb<-data_nb[index.data_nb,]
test.data_nb<-data_nb[-index.data_nb,]
model_nb<-naive_bayes(Purchase~.,train.data_nb)
pred<-predict(model_nb,test.data_nb)
accuracy(test.data_nb$Purchase,pred)
#Clone data into knn data variable
data_knn<-data
#Eliminate User_ID & Product_ID variables from the data set
data_knn<-subset(data_knn,select = -User_ID)
data_knn<-subset(data_knn,select = -Product_ID)
#load library dummies to convert categorical variables
library(dummies)
#Convert categorical variables into numerical using dummies
data_knn<-dummy.data.frame(data_knn,names=c("Gender","Age","Occupation","City_Category","Stay_In_Current_City_Years","Marital_Status"))
#Normalize the numeric variables
numeric.vars.knn = sapply(data_knn,is.numeric)
data_knn[numeric.vars.knn] <- lapply(data_knn[numeric.vars.knn],scale)
#Split the data into 80% training data and 20% testing data
set.seed(123)
test<-1:107515
train.data_knn<-data_knn[-test,]
test.data_knn<-data_knn[test,]
train.Purchase<-data_knn$Purchase[-test]
test.Purchase<-data_knn$Purchase[test]
#Load library class and Metrics to build KNN models, find predictions and compute accuracy
library(class)
library(Metrics)
#Build KNN Models for K=1,5,15
knn.1<-knn(train.data_knn,test.data_knn,train.Purchase,k=1)
accuracy(test.Purchase,knn.1)
knn.5<-knn(train.data_knn,test.data_knn,train.Purchase,k=5)
accuracy(test.Purchase,knn.5)
knn.15<-knn(train.data_knn,test.data_knn,train.Purchase,k=15)
accuracy(test.Purchase,knn.15)
#Clone data into lg data variable
data_lg<-data
#Eliminate User_ID & Product_ID variables from the data set
data_lg<-subset(data_lg,select = -User_ID)
data_lg<-subset(data_lg,select = -Product_ID)
#Split the data into 80% training data and 20% testing data
index.data_lg<-sample(1:nrow(data_lg),size=round(0.8*nrow(data_lg)))
train.data_lg<-data_lg[index.data_lg,]
test.data_lg<-data_lg[-index.data_lg,]
#Build full logistic regression model
model_lg_full<-glm(Purchase~.,data=train.data_lg,family=binomial())
summary(model_lg_full)
#Build base model for stepwise forward model
model_lg_base<-glm(Purchase~Product_Category_1,data=train.data_lg,family=binomial())
summary(model_lg_base)
#Build stepwise forward model
model_lg_fwd<-step(model_lg_base,scope=list(upper=model_lg_full,lower=~1),direction="forward",trace=T)
summary(model_lg_fwd)
#Build stepwise backward model
model_lg_bwd<-step(model_lg_full,direction="backward",trace=T)
summary(model_lg_bwd)
#AIC values of all the values are same find accuracy of full model
#load libraries caret and Metrics to use train() function to build full model, make predictions and compute accuracy
library(caret)
library(Metrics)
#Split the data into 80% training data and 20% testing data
train<-createDataPartition(data_lg$Purchase,p=0.8,list=FALSE)
training<-data_lg[train,]
testing<-data_lg[-train,]
data_lg[1,]
data_lg$Purchase<-as.factor(data_lg$Purchase)
#Build full logistic model using train()
mod_fit<-train(as.factor(Purchase)~Gender+Age+Occupation+City_Category+Stay_In_Current_City_Years+Marital_Status+Product_Category_1+Product_Category_2+Product_Category_3,data=training,method="glm",family="binomial")
pred<-predict(mod_fit,newdata=testing)
accuracy(pred,testing$Purchase)
#Validation of the best model = KNN
valid_pred<-knn.1
#Storing Model Performance Scores using the library ROCR
library(ROCR)
prediction_val<-prediction(as.numeric(knn.1),as.numeric(test.data_knn$Purchase))
#Calculating Area Under Curve(AUC)
performance_value<-performance(prediction_val,"auc")
#Plot AUC
preformance_value<-performance(prediction_val,"tpr","fpr")
plot(performance_value,col="red",lwd=1.5)
#Calculating KS statistics
AUC<-max(attr(performance_value,"y.values")[[1]]-(attr(performance_value,"x.values")[[1]]))
AUC
